{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This data set contains simulated data that mimics customer behavior on the Starbucks rewards mobile app. Once every few days, Starbucks sends out an offer to users of the mobile app. An offer can be merely an advertisement for a drink or an actual offer such as a discount or BOGO (buy one get one free). Some users might not receive any offer during certain weeks. \n",
    "\n",
    "Not all users receive the same offer, and that is the challenge to solve with this data set.\n",
    "\n",
    "Your task is to combine transaction, demographic and offer data to determine which demographic groups respond best to which offer type. This data set is a simplified version of the real Starbucks app because the underlying simulator only has one product whereas Starbucks actually sells dozens of products.\n",
    "\n",
    "Every offer has a validity period before the offer expires. As an example, a BOGO offer might be valid for only 5 days. You'll see in the data set that informational offers have a validity period even though these ads are merely providing information about a product; for example, if an informational offer has 7 days of validity, you can assume the customer is feeling the influence of the offer for 7 days after receiving the advertisement.\n",
    "\n",
    "You'll be given transactional data showing user purchases made on the app including the timestamp of purchase and the amount of money spent on a purchase. This transactional data also has a record for each offer that a user receives as well as a record for when a user actually views the offer. There are also records for when a user completes an offer. \n",
    "\n",
    "Keep in mind as well that someone using the app might make a purchase through the app without having received an offer or seen an offer.\n",
    "\n",
    "### Example\n",
    "\n",
    "To give an example, a user could receive a discount offer buy 10 dollars get 2 off on Monday. The offer is valid for 10 days from receipt. If the customer accumulates at least 10 dollars in purchases during the validity period, the customer completes the offer.\n",
    "\n",
    "However, there are a few things to watch out for in this data set. Customers do not opt into the offers that they receive; in other words, a user can receive an offer, never actually view the offer, and still complete the offer. For example, a user might receive the \"buy 10 dollars get 2 dollars off offer\", but the user never opens the offer during the 10 day validity period. The customer spends 15 dollars during those ten days. There will be an offer completion record in the data set; however, the customer was not influenced by the offer because the customer never viewed the offer.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "This makes data cleaning especially important and tricky.\n",
    "\n",
    "You'll also want to take into account that some demographic groups will make purchases even if they don't receive an offer. From a business perspective, if a customer is going to make a 10 dollar purchase without an offer anyway, you wouldn't want to send a buy 10 dollars get 2 dollars off offer. You'll want to try to assess what a certain demographic group will buy when not receiving any offers.\n",
    "\n",
    "### Final Advice\n",
    "\n",
    "Because this is a capstone project, you are free to analyze the data any way you see fit. For example, you could build a machine learning model that predicts how much someone will spend based on demographics and offer type. Or you could build a model that predicts whether or not someone will respond to an offer. Or, you don't need to build a machine learning model at all. You could develop a set of heuristics that determine what offer you should send to each customer (i.e., 75 percent of women customers who were 35 years old responded to offer A vs 40 percent from the same demographic to offer B, so send offer A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sets\n",
    "\n",
    "The data is contained in three files:\n",
    "\n",
    "* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n",
    "* profile.json - demographic data for each customer\n",
    "* transcript.json - records for transactions, offers received, offers viewed, and offers completed\n",
    "\n",
    "Here is the schema and explanation of each variable in the files:\n",
    "\n",
    "**portfolio.json**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - time for offer to be open, in days\n",
    "* channels (list of strings)\n",
    "\n",
    "**profile.json**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income\n",
    "\n",
    "**transcript.json**\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours since start of test. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "% matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read in the json files\n",
    "portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "#### Portfolio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(portfolio['offer_type']).plot.bar(title='Distribution of Offer Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(portfolio['duration']).plot.bar(title='Distribution of Duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "* From the above frequency charts, it is observed that BOGO and Discount are the most prevelant offer types in the portfolio datasets. \n",
    "* 7 days is the highest number of occurences for the number of days the offers are going to be live in the market\n",
    "\n",
    "#### Profile Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_values = (profile.isna().sum() * 100 / profile.shape[0]).sort_values(ascending=False)\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.barplot(missing_values[missing_values > 0].index, missing_values[missing_values > 0].values)\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,4))\n",
    "plt.subplot(1,2,1)\n",
    "profile.age.hist(bins = 30)\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Age Group Distribution')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "profile.income.hist(bins = 30);\n",
    "plt.xlabel('Income Range')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Income Range Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(profile['gender']).plot.bar(title='Distribution of Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "sns.boxplot(x=profile.gender, y =profile.age, data = profile, showfliers = False)\n",
    "plt.title('Gender vs Age distribution')\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Age distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "* Income and age has the highest missing values out of all the columns in the dataset\n",
    "* Some outlier age is present in the data which is more than 115\n",
    "* The average age of users is between 50-62 years in the dataset\n",
    "* Average income user is within the range 65000-70000\n",
    "* With respect to gender, the highest observations are for Males followed by females in the dataset\n",
    "* The median age for all genders is distributed within the range 55-60 in the data\n",
    "\n",
    "#### Transcript Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(transcript['event']).plot.bar(title='Distribution of Event Types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_person_value = transcript[transcript['event'] == 'offer received'].groupby(['person'], as_index = False).agg({'value':'count'})\n",
    "per_person_value.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "* On an average, 4 offers were received at a user level\n",
    "* The maximum number of offers received were 6 for a person\n",
    "* The different types of events in the transcript dataseta are transaction, offer received, offer viewed and offer completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "#### Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(df, new_cols_name):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    df: input dataframe for renaming columns\n",
    "    new_cols_name: define new column name for each column\n",
    "    \n",
    "    Output\n",
    "    df: output data frame with renamed column names\n",
    "    \"\"\"\n",
    "    df= df.rename(columns = new_cols_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_renamed_cols = {'difficulty':'offer_difficulty' , 'id':'offer_id', 'duration':'offer_duration', 'reward': 'offer_reward'}\n",
    "portfolio_renamed = rename_column(portfolio, portfolio_renamed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_renamed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_renamed_cols = {'id':'customer_id' , 'income':'customer_income'}\n",
    "profile_renamed = rename_column(profile, profile_renamed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_renamed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_clean(df):\n",
    "    \"\"\"        \n",
    "    Input   \n",
    "    df: data frame\n",
    "    \n",
    "    Output\n",
    "    df: output data frame cleaned for missing values and other inconsistencies\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace 118 age values with NaN and then in turn replace them with the mean of the age column\n",
    "    df.replace(118, np.nan , inplace=True)\n",
    "    \n",
    "    # Replace NaN age values with average of the age column\n",
    "    df['age'] = df['age'].fillna(df['age'].mean())\n",
    "    \n",
    "    # Replace missing income values with average of the income column\n",
    "    df['customer_income'] = df['customer_income'].fillna(df['customer_income'].mean())\n",
    "    \n",
    "    # Replace missing gender values with mode of the gender column\n",
    "    mode = df['gender'].mode()[0]\n",
    "    df['gender'] = df['gender'].fillna(mode)\n",
    "    \n",
    "    # Remove outliers from the age column and change the data type to int\n",
    "    df = df[df['age'] <= 80]\n",
    "    df['age'] = df['age'].astype(int)\n",
    "    \n",
    "    # Convert the age column to categorical\n",
    "    df.loc[(df.age < 20) , 'age_group'] = 'Under 20'\n",
    "    df.loc[(df.age >= 20) & (df.age <= 45) , 'age_group'] = '20-35'\n",
    "    df.loc[(df.age >= 36) & (df.age <= 45) , 'age_group'] = '36-45'\n",
    "    df.loc[(df.age >= 46) & (df.age <= 60) , 'age_group'] = '46-60'\n",
    "    df.loc[(df.age >= 61) , 'age_group'] = '61-80'\n",
    "    df.drop('age',axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_clean = profile_clean(profile_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_renamed_cols = {'person':'customer_id'}\n",
    "transcript_renamed = rename_column(transcript, transcript_renamed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_renamed.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_clean(df):\n",
    "    \"\"\"\n",
    "    Input   \n",
    "    df: data frame\n",
    "    \n",
    "    Output\n",
    "    df: output data frame cleaned and parsed for the dictionary column 'value'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expand the dictionary based column to singular columns\n",
    "    df['offer_id'] = df['value'].apply(lambda x: x.get('offer_id'))\n",
    "    df['offer id'] = df['value'].apply(lambda x: x.get('offer id'))\n",
    "    df['money_gained'] = df['value'].apply(lambda x: x.get('reward'))\n",
    "    df['money_spent'] = df['value'].apply(lambda x: x.get('amount'))\n",
    "    \n",
    "    # Move 'offer id' values into 'offer_id'\n",
    "    df['offer_id'] = df.apply(lambda x : x['offer id'] if x['offer_id'] == None else x['offer_id'], axis=1)\n",
    "    \n",
    "    # Drop 'offer id' column \n",
    "    df.drop(['offer id' , 'value'] , axis=1, inplace=True)\n",
    "    \n",
    "    # Replace NANs\n",
    "    df.fillna(0 , inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_clean = transcript_clean(transcript_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Performing some EDA on the merged datasets after having cleaned for inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(portfolio_renamed, transcript_clean, on='offer_id')\n",
    "merged_df = pd.merge(merged_df, profile_clean, on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.customer_income.hist(bins = 30)\n",
    "plt.xlabel('Income Brackets')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Customer Income Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(merged_df['offer_type']).plot.bar(title='Distribution of Offer Types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(merged_df['age_group']).plot.bar(title='Distribution of Age Groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(merged_df['event']).plot.bar(title='Distribution of Offer Actions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "* In the offer type section of the dataset, BOGO and Discount are the most frequent\n",
    "* It is seen that the most engagement is within the age groups 46-60 followed by 61-80, i.e., anyone above the age 46 is highly engaging with our brand\n",
    "* It is observed that the conversion from offer received to offer completed is roughly about 42% of all that received offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x= \"offer_type\", hue= \"age_group\", data=merged_df)\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.title('Age Group Distribution in Offer Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Offer Type')\n",
    "plt.legend(title='Age Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x= \"event\", hue= \"age_group\", data=merged_df)\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.title('Age Group Distribution in events')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Event')\n",
    "plt.legend(title='Age Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x= \"event\", hue= \"offer_type\", data=merged_df)\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.title('Distribution of offer types in events')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Event')\n",
    "plt.legend(title='Offer Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "* Again, age group equal and above 46 are the ones most engaged in terms of any sort of offers\n",
    "* Even for the event engagement with the offers, the highest activity is prominent in this age group, while under 20 is the least active. This indicates that the adult and senior citizens are the most engaging with any programs that Starbucks has to roll out\n",
    "* The funnel from offer received to offer completed looks promising for Bogo offers followed by discounts, while informational offers do not see any offer completions at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model\n",
    "\n",
    "We build a machine learning model to predict the response of a customer to an offer.\n",
    "\n",
    "#### Clean the data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Input   \n",
    "    df: data frame\n",
    "    \n",
    "    Output\n",
    "    df: cleaned data frame with the below steps\n",
    "    \"\"\"\n",
    "    # One-hot encode categorical features\n",
    "    cat_features = ['offer_type', 'gender', 'age_group']\n",
    "    df = pd.get_dummies(df, columns = cat_features)\n",
    "    \n",
    "    # Separate the channels in the channel column and transfor using one-hot encoding\n",
    "    df = df.drop('channels', 1).join(df.channels.str.join('|').str.get_dummies())\n",
    "    \n",
    "    # Change data type of required features\n",
    "    df['became_member_on'] = df['became_member_on'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "    \n",
    "    # Extract month and year from became_member_on column\n",
    "    df['month_member'] = df['became_member_on'].apply(lambda x: x.day)\n",
    "    df['year_member'] = df['became_member_on'].apply(lambda x: x.year)\n",
    "    \n",
    "    # Drop original feature\n",
    "    df.drop('became_member_on',axis=1, inplace=True)\n",
    "    \n",
    "    # Fix the offer_id feature\n",
    "    offerid_list = df['offer_id'].unique().tolist()\n",
    "    offer_mapping = dict(zip(offerid_list, range(len(offerid_list))) )\n",
    "    df.replace({'offer_id': offer_mapping}, inplace=True)\n",
    "    \n",
    "    # Fix customer_id feature\n",
    "    customer_ids = df['customer_id'].unique().tolist()\n",
    "    customer_mapping = dict(zip(customer_ids,range(len(customer_ids))) )\n",
    "    df.replace({'customer_id': customer_mapping}, inplace=True)\n",
    "    \n",
    "    # Scale the continuous features\n",
    "    scaler = MinMaxScaler()\n",
    "    num_features = ['customer_income', 'offer_difficulty', 'offer_duration', 'offer_reward', \n",
    "                    'time', 'money_gained', 'money_spent']\n",
    "    df[num_features] = scaler.fit_transform(df[num_features])\n",
    "    \n",
    "    # Encode 'event' data to numerical values according to task 2\n",
    "    df['event'] = df['event'].map({'offer received':1, 'offer viewed':2, 'offer completed':3})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = clean_data(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_df.drop('event', axis=1)\n",
    "label = final_df['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size = 0.2, random_state = 7)\n",
    "\n",
    "print(\"Training set: {} rows\".format(X_train.shape[0]))\n",
    "print(\"Testing set: {} rows\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(clf):\n",
    "    \"\"\"\n",
    "    Train and Test F1 score along with the model name\n",
    "     \n",
    "    Input\n",
    "    clf: Estimator Instance\n",
    "    \n",
    "    Output\n",
    "    Train data F1 score\n",
    "    Test data F1 score\n",
    "    Model name\n",
    "       \n",
    "    \"\"\"\n",
    "    train_prediction = (clf.fit(X_train, y_train)).predict(X_train)\n",
    "    test_predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "    train_f1 =  accuracy_score(y_train, train_prediction) * 100\n",
    "    test_f1= fbeta_score(y_test, test_predictions, beta = 0.5, average='micro' ) * 100\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    return train_f1, test_f1, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighbors_clf = KNeighborsClassifier(n_neighbors = 3)\n",
    "KNeighbors_train_f1, KNeighbors_test_f1, KNeighbors_model = scores(KNeighbors_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = {'Benchmark Model': [KNeighbors_model], 'train F1 score':[KNeighbors_train_f1], 'test F1 score': [KNeighbors_test_f1]}\n",
    "benchmark = pd.DataFrame(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree_clf = DecisionTreeClassifier(random_state = 10)\n",
    "DecisionTree_train_f1, DecisionTree_test_f1, DecisionTree_model = scores(DecisionTree_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = {'Benchmark Model': [DecisionTree_model], 'Train F1 score':[DecisionTree_train_f1], 'Test F1 score': [DecisionTree_test_f1]}\n",
    "benchmark = pd.DataFrame(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_clf = RandomForestClassifier(random_state = 10)\n",
    "RandomForest_train_f1, RandomForest_test_f1, RandomForest_model = scores(RandomForest_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = {'Benchmark Model': [RandomForest_model], 'Train F1 score':[RandomForest_train_f1], 'Test F1 score': [RandomForest_test_f1]}\n",
    "benchmark = pd.DataFrame(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression_clf = LogisticRegression(random_state = 10)\n",
    "LogisticRegression_train_f1, LogisticRegression_test_f1, LogisticRegression_model = scores(LogisticRegression_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = {'Benchmark Model': [LogisticRegression_model], 'Train F1 score':[LogisticRegression_train_f1], 'Test F1 score': [LogisticRegression_test_f1]}\n",
    "benchmark = pd.DataFrame(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_combined = {'Model': ['KNeighborsClassifier (Benchmark)', DecisionTree_model, RandomForest_model, LogisticRegression_model], \n",
    "          'Train F1 score ':[KNeighbors_train_f1, DecisionTree_train_f1, RandomForest_train_f1, LogisticRegression_train_f1], \n",
    "          'Test F1 score': [KNeighbors_test_f1 , DecisionTree_test_f1, RandomForest_test_f1, LogisticRegression_test_f1] }\n",
    "          \n",
    "model_comparison = pd.DataFrame(model_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "* The test set has been used to evaluate the model performances. K Neighbours was set as the benchmark model and the other model scores were compared to the same\n",
    "* Decision Tree Classifier had the best validation F1 score of all (84.6%) while the benchmark was only 25%\n",
    "* While the Random Forest Classifier had a remarkable 93% on the Train set, the scores for the validation set turned out to be 67% only\n",
    "* Even though there has been variabilities in the F1 scores for Train and Test sets, it is still fine to use the models to predict if a customer will respond to an offer or not\n",
    "* We can further use hyper-parameter tuning to improve on the scores, as future scope of this project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
